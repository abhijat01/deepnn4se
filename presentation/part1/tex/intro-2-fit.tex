\section{Fitting functions}

\begin{frame}{Fitting a function to data}
		\begin{block}{Description (supervised learning)}
		Given a set of data $(x_1,y_1),(x_2,y_2), \dots, (x_n,y_n)$, can 
		we find a function $y=f(x)$ that "fits" this data? 		
		\end{block}		
		\begin{block}{Questions}
			\begin{itemize}
				\item What is this function $f(x)$?
				\item What does "fit" mean? 
				\item How do we know this works? 
				\item What kinds of problems can we solve? 
			\end{itemize}
		\end{block}
\end{frame}

\begin{frame}{More about $f(x)$}
	\begin{block}{Class of functions} 
		Starting with a function $f(x;\theta_1,\theta_2,\dots, \theta_n)$
		where $x$ is the input to the function and $\theta s$ are its parameters, 
		we need to find the set of $\theta s$ that best "fits" the give 
		data $(x_1,y_1),(x_2,y_2), \dots, (x_n,y_n)$ 
		
	\end{block}
	\begin{block}{Class of linear functions}
		Consider  $f(x) = ax+b$. If we can say, with some confidence, that 
		our data is linearly related,  we need to find  
		$\theta_1=a$, $\theta_2=b$ that {\it fits} the given data. 
		We can also write it as $f(x;a,b) = ax+b$. 
	\newline 
	Preferred,  
	\begin{align}
	f(x;\theta_1,\theta_2) &= \theta_1x+\theta_2	
	\end{align}

	\end{block}

\end{frame}
\begin{frame}{Fit}
	\begin{block}{Mean squared Euclidean distance as one possible measure of fit}
		Let $L_i$ be the squared Euclidean distance between the predicted 
		value, $\hat{y_i}=f(x_i)$ and the actual, $y_i$. Then,  
		\begin{align}
		L_i &=z_i^2 \\
		z_i &=y_i-f(x_i)\\
		    &=y_i-\theta_1 x_i-\theta_2 
		\end{align}
		Minimizing $L_i$ with respect to the parameters $\theta_1$ and $\theta_2$,
		\begin{align}
		\frac{\partial L_i}{\partial \theta_1} &= 
		\frac{\partial z_i^2}{\partial z_i} \; 
		\frac{\partial z_i}{\partial \theta_1}  \\
		\frac{\partial L_i}{\partial \theta_2} &= 
		\frac{\partial z_i^2}{\partial z_i} \; 
		\frac{\partial z_i}{\partial \theta_2} 
		\end{align}
	\end{block}
\end{frame}

\begin{frame}%{Mean squared distance and fit}
	For $n$ data points, mean  loss is 
	\begin{align*}
		L = \frac{1}{n} \sum_{i=1}^{i=n}L_i  
		  = \frac{1}{n} \sum_{i=1}^{i=n} (y_i-\theta_1 x_i-\theta_2)^2
	\end{align*}
	{\it In practice, we choose a much smaller subset of data, called a batch, compute 
	mean loss over this batch and run  optimization step using the gradient of loss 
	(more on this later).} 
	\begin{block}{Two different spaces}
		\begin{itemize}
			\item Space spanned by $x$ and $y$. Optimization tries to find the surface that best 
			fits the data. 
			\item Space spanned by $\theta s$. We minimize the loss function in this space.  
		\end{itemize}
	\end{block}
\end{frame}


