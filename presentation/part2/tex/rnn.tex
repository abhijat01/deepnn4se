\section{Sequences}

\begin{frame}{Sequences} 
	%\begin{columns}
	%	\begin{column}{.5\textwidth}
			\begin{itemize}
				\item Natural language tasks 
				\item Event processing  
				\item Stateful systems in general
			\end{itemize}	
	%	\end{column}
	%	\begin{column}{.5\textwidth}
		\begin{block}{Types}
			\begin{itemize}
				\item Variable length sequences, all elements known ahead of time. 
				\item Constant length sequences, all elements known ahead of time. 
				\item Constant length sequences, revealed one element at a time.
				\item Variable length sequences, revealed one element at a time.
			\end{itemize}
		\end{block}

	%	\end{column}
	%\end{columns}
\end{frame}

\begin{frame}{Note about working with sentences}
\begin{itemize}
	\item One-hot encodings
		\begin{itemize}
			\item[-] Very long vectors 
			\item[-] Vectors do not represent relationship between words.
		\end{itemize} 
	\item Word embeddings, e.g., word2vec  
	\begin{itemize}
		\item[-] Uses a sliding window to predict a word given surrounding context
		\item[-] Captures some key relationship between words. 
		\item[-] Given enough data, models often learn embeddings.  
	\end{itemize}
	\item Sub-word models 
		\begin{itemize}
			\item[-] Works better with social media and informal speech
			\item[-] Languages that may "create" new words on the fly (Sanskrit).
		\end{itemize} 
\end{itemize}
\end{frame}

\begin{frame}{Functions of form $y_t = f(y_{t-1}, x_t; \theta)$}

\begin{eqnarray}
I_t =& UX+Vh_{t-1}+b \\
h_t =& tanh(I_t) \\
O_t =& Wh_t + c 
\end{eqnarray}

\begin{center}
\begin{figure}
	\includegraphics[width=.8\textwidth]{figures/rnn_wikipedia_1}
	\caption*{\tiny{"Recurrent neural network" (2020) Wikipedia. Available at:
	https://en.wikipedia.org/wiki/Recurrent\_neural\_network}}
\end{figure}
\end{center}
\end{frame}

\begin{frame}{Recurrent network architecture styles}
\begin{center}
	\begin{figure}
		\includegraphics[width=1\textwidth]{figures/rnn_seq_2_seq}
		\caption*{{Sequence to sequence network}}
	\end{figure}
\end{center}
\end{frame}
\begin{frame}{Recurrent network architecture styles}
\begin{center}
	\begin{figure}
		\includegraphics[width=1\textwidth]{figures/rnn_seq_2_vec}
		\caption*{{Sequence to vector network}}
	\end{figure}
\end{center}
\end{frame}
\begin{frame}{Recurrent network architecture styles}
\begin{center}
	\begin{figure}
		\includegraphics[width=1\textwidth]{figures/rnn_vec_2_seq}
		\caption*{{Vector to sequence network}}
	\end{figure}
\end{center}
\end{frame}
\begin{frame}{Recurrent network architecture styles}
\begin{center}
	\begin{figure}
		\includegraphics[width=1\textwidth]{figures/rnn_seq_2_seq_encoder_decoder}
		\caption*{{Sequence to sequence encoder-decoder network}}
	\end{figure}
\end{center}
\end{frame}